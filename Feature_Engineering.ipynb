{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "OloukaVEGQaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. What is a parameter?"
      ],
      "metadata": {
        "id": "vMB9Fa6-GTqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A parameter is a variable listed inside the parentheses in a function definition. It acts as a placeholder for values (called arguments) passed into the function when it is called."
      ],
      "metadata": {
        "id": "waP72gJ-Gt_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "qHUF28GaHUwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Correlation is a statistical measure that shows how two variables move in relation to each other.\n",
        "\n",
        " A negative correlation, also known as an inverse correlation, describes a relationship between two variables where an increase in one variable is associated with a decrease in the other, and vice versa. In simpler terms, when one thing goes up, the other goes down.  Example: If time spent exercising increases, body fat percentage might decrease."
      ],
      "metadata": {
        "id": "NPib47hlHkYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "TLXjiewuIdvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "Main Components of Machine Learning:\n",
        "1. Data\n",
        "\n",
        "  The foundation of ML. Algorithms learn patterns from structured or unstructured data.\n",
        "\n",
        "2. Model\n",
        "\n",
        "  A mathematical representation created by the algorithm that captures the patterns in the data.\n",
        "\n",
        "3. Algorithm\n",
        "\n",
        "  The method or set of rules the model follows to learn from data (e.g., Linear Regression, Decision Trees, etc.).\n",
        "\n",
        "4. Training\n",
        "\n",
        "  The process of feeding data to the model so it can learn patterns.\n",
        "\n",
        "5. Testing\n",
        "\n",
        "  After training, the model is tested on new/unseen data to check how well it performs.\n",
        "\n",
        "6. Features\n",
        "\n",
        "  The input variables used by the model to make predictions (e.g., age, income, height).\n",
        "\n",
        "7. Labels (Target)\n",
        "\n",
        "  The output the model is trying to predict (e.g., price, yes/no, category)."
      ],
      "metadata": {
        "id": "hA00oexZIhbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "WslvKsi7I6eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Loss value serves as a crucial indicator of model performance. A lower loss value generally suggests better model accuracy, while a higher loss value indicates a model that is making significant errors. The loss function quantifies the difference between the model's predictions and the actual values, and the goal during training is to minimize this loss."
      ],
      "metadata": {
        "id": "IwqXlTuLI8k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "PQCoaP2EJNFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> continuous variables represent measurable quantities that can take on any value within a range, while categorical variables represent data that can be divided into distinct groups or categories.\n",
        "\n",
        "Continuous variables are typically numerical and can have values that can be measured with a high degree of precision (e.g., height, weight)\n",
        "\n",
        "Categorical variables, on the other hand, are not numerical and represent different categories or groups (e.g., gender, eye color, race, city of residence)."
      ],
      "metadata": {
        "id": "FRrOw_uGJQmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "ZX-bWHprKBBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Categorical variables in machine learning need to be converted into numerical representations because most algorithms only accept numerical inputs. This is done through various encoding techniques, each suitable for different types of categorical data and model requirements."
      ],
      "metadata": {
        "id": "0LKZHKcwKVlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "6LmBAJ2oKj49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A dataset is a collection of data used to train and evaluate a machine learning model. It's usually divided into two main parts: training set and testing set.\n",
        "\n",
        "1. Training Dataset\n",
        "\n",
        "  - Used to teach the model.\n",
        "\n",
        "  - The model learns patterns from this data.\n",
        "\n",
        "  - It sees both input features and the correct answers (labels).\n",
        "\n",
        "2.  Testing Dataset\n",
        "\n",
        "  - Used to evaluate how well the model has learned.\n",
        "\n",
        "  - This data is not shown to the model during training.\n",
        "\n",
        "  - Helps test the model’s ability to generalize to unseen data.  "
      ],
      "metadata": {
        "id": "hwEJSJk-Knm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. What is sklearn.preprocessing?\n"
      ],
      "metadata": {
        "id": "tLiG7WI1LRks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, learning algorithms benefit from standardization of the data set."
      ],
      "metadata": {
        "id": "3-xI7Im-LTS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. What is a Test set?\n"
      ],
      "metadata": {
        "id": "1IhpxEfdLwCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A test set is a portion of the dataset that is used to evaluate the performance of a trained machine learning model. It contains unseen data that the model did not learn from during training."
      ],
      "metadata": {
        "id": "KoiPBqCMLzAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "ryMMWMPeL5_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In Python, you typically split data for training and testing using the train_test_split function from the scikit-learn library. This function randomly divides your data into two subsets: one for training the model and the other for evaluating its performance. The typical approach to a Machine Learning problem involves data collection, preparation, model selection, training, evaluation, and potentially parameter tuning before making predictions.\n",
        "\n",
        "```\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X is your features and y is your target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 80% training, 20% testing\n",
        "\n",
        "```\n",
        "\n",
        "How to Approach a Machine Learning Problem:\n",
        "\n",
        "1. Understand the Problem:\n",
        "\n",
        "- What is the goal? Is it a regression (predicting continuous values) or classification (predicting categories)?\n",
        "\n",
        "- Identify the input variables (features) and the target variable (what you want to predict).\n",
        "\n",
        "2. Collect and Clean Data:\n",
        "\n",
        "- Collect data from reliable sources.\n",
        "\n",
        "- Clean the data: Handle missing values, duplicates, and outliers.\n",
        "\n",
        " - Transform features if necessary (e.g., encoding categorical variables, scaling numerical data).\n",
        "\n",
        "3. Explore Data (EDA):\n",
        "\n",
        "- Use visualization and statistical summaries to understand relationships between features and the target.\n",
        "\n",
        "- Look for patterns, correlations, or potential issues in the data.\n",
        "\n",
        "4. Split the Data:\n",
        "\n",
        "- Split into training and test sets to ensure the model is evaluated properly on unseen data.\n",
        "\n",
        "5. Choose a Model:\n",
        "\n",
        "- Select a machine learning model based on the problem (e.g., linear regression, decision trees, random forests, etc.).\n",
        "\n",
        "- Consider trying different models and comparing their performance.\n",
        "\n",
        "6. Train the Model:\n",
        "\n",
        "- Use the training data to teach the model by fitting it.\n",
        "\n",
        "- Fine-tune model hyperparameters using cross-validation if needed.\n",
        "\n",
        "7. Evaluate the Model:\n",
        "\n",
        "- Use the test set to check the model's performance (accuracy, precision, recall, RMSE, etc.).\n",
        "\n",
        "- Look at evaluation metrics to determine how well the model generalizes to new data.\n",
        "\n",
        "8. Improve the Model:\n",
        "\n",
        "- If performance is poor, try different models, use more features, tune hyperparameters, or gather more data.\n",
        "\n",
        "9. Deploy and Monitor:\n",
        "\n",
        "- Once satisfied with the model, deploy it for real-world use.\n",
        "\n",
        "- Continuously monitor the model's performance, as data might change over time (model drift).\n",
        "\n"
      ],
      "metadata": {
        "id": "jsN0IPcyMEaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "py9NkJKENZPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->\n",
        "1. Understand the Data\n",
        "\n",
        "  - EDA helps you get a sense of the dataset: its structure, features, and relationships between variables.\n",
        "\n",
        "  - Without EDA, you may not know if there are outliers, missing values, or other data quality issues that could negatively affect your model.\n",
        "\n",
        "\n",
        "2. Feature Relationships\n",
        "\n",
        "- EDA helps identify relationships between input features and the target variable.\n",
        "\n",
        "- You can visualize correlations to see which features are likely relevant for predicting the target and which may be irrelevant.\n",
        "\n",
        "3. Detect Outliers\n",
        "\n",
        "- Outliers can skew the model's predictions, so detecting them early is crucial.\n",
        "\n",
        "- EDA lets you identify if there are any extreme values in your data that could impact the model.\n",
        "\n",
        "4. dentify Data Imbalance\n",
        "- Class imbalance (for classification problems) can occur if one class is significantly more frequent than the other.\n",
        "\n",
        "- EDA helps you identify this imbalance so you can apply techniques (like oversampling or undersampling) to balance the dataset.\n",
        "\n",
        "5. Feature Engineering\n",
        "- EDA helps you create new features or decide which existing features to drop.\n",
        "\n",
        "- For example, if you find that the age feature is skewed, you might decide to transform it into a new feature like age group (e.g., 0-18, 19-30, etc.).\n",
        "\n",
        "6. Choose the Right Model\n",
        "- Based on the insights from EDA, you can choose an appropriate model.\n",
        "\n",
        "- For example, if your data has linear relationships, you might choose linear regression. If it has complex, non-linear relationships, you might opt for tree-based models like Random Forest or XGBoost.\n",
        "\n"
      ],
      "metadata": {
        "id": "MeiYPjpiNchb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12. What is correlation?"
      ],
      "metadata": {
        "id": "7saXv_hhOrW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Correlation is a statistical measure that describes the relationship between two variables. It shows whether and how strongly the variables are related to each other."
      ],
      "metadata": {
        "id": "x3vVA2i0O6h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "ama1AdtTPH59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A negative correlation means that as one variable increases, the other variable decreases, and vice versa. In other words, the two variables move in opposite directions.\n",
        "\n"
      ],
      "metadata": {
        "id": "aKZSMvS0PJ72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "-taYSpwlPaVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> To use the corrcoef() function, you need to pass in two arrays of data, one for each variable. The function will return a correlation matrix, which is a square matrix where the diagonal elements are always 1 and the off-diagonal elements indicate the correlations between different variables.  The correlation coefficient is determined by dividing the covariance by the product of the two variables' standard deviations."
      ],
      "metadata": {
        "id": "Fx_mXkqzPeF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "E0HItt8qP7fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Causation (or causal relationship) refers to a situation where one variable directly influences or causes a change in another variable. In other words, X causes Y.  Causal relationships imply a cause-and-effect scenario where the change in one variable leads to the change in the other variable.\n",
        "\n",
        "1. Correlation Example:\n",
        "Ice Cream Sales & Drowning Incidents\n",
        "\n",
        "- Observation: There’s a correlation between ice cream sales and drowning incidents. Both increase in the summer.\n",
        "\n",
        "- Correlation: As ice cream sales go up, drowning incidents also increase.\n",
        "\n",
        "  - But: This does not mean that eating ice cream causes drowning. The true underlying factor is the season (summer), when more people swim and also eat ice cream.\n",
        "\n",
        "  - Conclusion: This is a spurious correlation. Ice cream sales and drowning incidents are both influenced by warmer weather, but one doesn't cause the other.\n",
        "\n",
        "2.  Causation Example:\n",
        "\n",
        "Smoking & Lung Cancer\n",
        "\n",
        "- Observation: There is strong evidence that smoking causes lung cancer.\n",
        "\n",
        "- Causation: The chemicals in cigarettes directly damage lung tissue, increasing the risk of cancer.\n",
        "\n",
        "  - Conclusion: Smoking is the cause of lung cancer, not just correlated with it.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "UAlFHZBKQD0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "nTzxk46xRC1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> An optimizer in machine learning is an algorithm or method used to minimize or maximize a function (typically a loss function) by adjusting the model's parameters (weights and biases) during training. The goal is to find the optimal values for these parameters to minimize the error between the predicted and actual values.\n",
        "\n",
        "Types of Optimizers\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "How it works: The optimizer adjusts parameters by taking steps proportional to the negative of the gradient (partial derivatives) of the loss function with respect to the model parameters. It moves downhill on the loss curve to find the minimum.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "How it works: Unlike batch gradient descent, SGD updates the model parameters after evaluating each individual data point. This makes it faster and more computationally efficient.\n",
        "\n",
        "3. Momentum\n",
        "How it works: Momentum helps accelerate gradient descent by adding a \"momentum\" term that takes into account the previous updates to smooth out oscillations. It speeds up convergence by accumulating previous gradients and using them to influence the current update.\n",
        "\n",
        "4. Adagrad\n",
        "How it works: Adagrad adapts the learning rate based on the frequency of updates. It gives larger updates to less frequent features and smaller updates to frequently occurring features.\n",
        "\n",
        "5. RMSprop (Root Mean Square Propagation)\n",
        "How it works: RMSprop is similar to Adagrad but solves its problem of rapidly decaying learning rates. It divides the gradient by the moving average of its recent magnitudes.\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "How it works: Adam combines the advantages of both Momentum and RMSprop. It keeps track of both the first moment (mean) and the second moment (variance) of the gradients, adapting the learning rates for each parameter."
      ],
      "metadata": {
        "id": "OrY8quHYRTwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "yEWz89b6SjwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> The sklearn.linear_model module in Scikit-learn (a popular machine learning library) provides various linear models for regression and classification tasks. These models are based on the concept of a linear relationship between the input features and the output target."
      ],
      "metadata": {
        "id": "oic3KrBNSrIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "5C0tX7WMTBg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In Scikit-learn, the model.fit() method is used to train the model on the given data. It learns the relationship between the input features (X) and the target variable (y) by adjusting the internal parameters (e.g., weights and biases for linear models).\n",
        "\n",
        "- In Regression: It learns how to predict a continuous target variable based on input features.\n",
        "\n",
        "- In Classification: It learns how to classify data points into predefined categories.\n",
        "\n",
        "Essentially, the fit() method finds the best model parameters (e.g., weights) that minimize the loss function for the given data."
      ],
      "metadata": {
        "id": "I5IuuAxHTErk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "uEex67CQTRbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> The model.predict() method in Scikit-learn is used to make predictions on new, unseen data after a model has been trained using the fit() method. Essentially, it takes the input features and outputs the predicted target values based on the relationships learned during training.\n",
        "\n",
        "- For Regression: The model predicts continuous values (e.g., prices, quantities).\n",
        "\n",
        "- For Classification: The model predicts discrete class labels (e.g., category 1, category 2).\n",
        "\n",
        "In other words, after training the model with the training data (using fit()), you can use predict() to apply the model to make predictions for new data.\n",
        "\n",
        "Arguments for model.predict()\n",
        "\n",
        "The model.predict() method typically requires one argument:\n",
        "\n",
        " 1. X (Features):\n",
        "\n",
        "- This is the input data you want to make predictions on. It should have the same structure as the training data used in fit().\n",
        "\n",
        "- Shape: X should be a 2D array or DataFrame with shape (n_samples, n_features), where:\n",
        "\n",
        "  - n_samples is the number of new data points you want to predict for.\n",
        "\n",
        "  - n_features is the number of features (the same as in the training data).\n",
        "\n",
        "- Example: If you are predicting house prices, X might contain the features such as square footage, number of rooms, etc., for the new houses you want to predict prices for.\n",
        "\n",
        "Syntax of model.predict()\n",
        "\n",
        "    predictions = model.predict(X)\n"
      ],
      "metadata": {
        "id": "y65HGJbYTTEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "T1eDnAUfUtXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In data analysis and machine learning, variables (also called features or columns) are usually categorized as continuous or categorical, depending on the type of data they contain.\n",
        "\n",
        "1. Continuous Variables\n",
        "\n",
        "  Definition:\n",
        "A continuous variable is a numerical variable that can take any value within a range. These values are typically measurable and can include decimals.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Height (e.g., 170.2 cm)\n",
        "\n",
        "- Weight (e.g., 65.5 kg)\n",
        "\n",
        "- Temperature (e.g., 22.3°C)\n",
        "\n",
        "- Age (e.g., 21.5 years)\n",
        "\n",
        "- Salary, price, distance, time, etc.\n",
        "\n",
        "Key characteristics:\n",
        "\n",
        "- Can have infinite values within a given range.\n",
        "\n",
        "- Can be mathematically operated on (addition, mean, standard deviation).\n",
        "\n",
        "- Often used in regression problems.\n",
        "\n",
        "2. Categorical Variables\n",
        "  \n",
        "  Definition:\n",
        "A categorical variable contains values that represent categories or groups. These values are usually labels or names, not numbers used for calculations.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- Gender (Male, Female, Other)\n",
        "\n",
        "- Color (Red, Green, Blue)\n",
        "\n",
        "- Country (India, USA, UK)\n",
        "\n",
        "- Product type (Laptop, Phone, Tablet)\n",
        "\n",
        "Key characteristics:\n",
        "\n",
        "- Values represent categories, not quantities.\n",
        "\n",
        "- Can be:\n",
        "\n",
        "  - Nominal: No natural order (e.g., Color: Red, Blue, Green)\n",
        "\n",
        "  - Ordinal: Has a logical order (e.g., Education Level: High School < Bachelor < Master < PhD)\n",
        "\n",
        "- Often used in classification problems.\n"
      ],
      "metadata": {
        "id": "kGc2ZkSgUwZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "03Sj-CZjV8fJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Feature scaling is a data preprocessing technique used in Machine Learning to bring all the input features (variables) onto a similar scale—usually so that no one feature dominates the others just because of its range of values.\n",
        "\n",
        "In many ML algorithms, the model measures distances or optimizes weights during training. If the features are on very different scales, the model might:\n",
        "\n",
        "- Give more importance to features with larger numerical ranges,\n",
        "\n",
        "- Struggle to converge properly, or\n",
        "\n",
        "- Make poor predictions due to imbalance.\n",
        "\n",
        "Benefits of Feature Scaling\n",
        "- Speeds up training of models\n",
        "\n",
        "- Helps gradient descent converge faster\n",
        "\n",
        "- Improves model performance (especially for distance-based models)\n",
        "\n",
        "- Prevents bias toward features with larger values"
      ],
      "metadata": {
        "id": "TROZUwgFWB9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "ZE67RgjBW1Qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> To scale features in Python, we commonly use Scikit-learn's preprocessing module. The most used scaling methods are:\n",
        "\n",
        "- Min-Max Scaling\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1], [5], [10], [15]])\n",
        "\n",
        "# Apply Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "\n",
        "```\n",
        "- Standardization (Z-score Scaling)\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1], [5], [10], [15]])\n",
        "\n",
        "# Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "\n",
        "```\n",
        "- Robust Scaling\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1], [5], [10], [100]])\n",
        "\n",
        "# Apply Robust Scaling\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OJmnbLAZW-9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "gYadCxcnX0In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> sklearn.preprocessing is a module in Scikit-learn that provides tools to prepare and transform raw data into a format that can be used effectively by machine learning models."
      ],
      "metadata": {
        "id": "Qzzm-FOQX3_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "tkL3yB4IYDqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> To split data for model fitting, training, and testing in Python, you can use the train_test_split function from the scikit-learn library. This function randomly divides your dataset into training and testing sets, allowing you to train your model on a portion of the data and evaluate its performance on unseen data."
      ],
      "metadata": {
        "id": "Ts6kmrKXYFkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##25. Explain data encoding?"
      ],
      "metadata": {
        "id": "6oxkREozYYRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Data encoding is the process of converting categorical data (text or labels) into a numeric format so it can be used by machine learning algorithms.\n",
        "\n",
        "Why Is Encoding Important?\n",
        "- Many datasets have categorical features like Gender, Country, Color, etc.\n",
        "\n",
        "- Algorithms work with numerical inputs only.\n",
        "\n",
        "- Encoding helps translate labels into numbers while preserving meaning."
      ],
      "metadata": {
        "id": "AZd6IyntYbBT"
      }
    }
  ]
}